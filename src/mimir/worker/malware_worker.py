"""Malware feed connector worker.

Standalone worker process that periodically syncs malware sample data
from Elasticsearch indices (``mwdb-openrelik``, ``dailymalwarefeed``)
into the Mimir knowledge graph.

The worker:
* Runs on its own schedule (``MALWARE_WORKER_INTERVAL_MINUTES``, default 30)
* Uses a lookback window (``MALWARE_WORKER_LOOKBACK_MINUTES``) to catch
  missed samples
* Directly extracts structured entities (hashes, families, ATT&CK
  techniques, IOCs, Yara rules) without LLM processing
* Gracefully shuts down on SIGINT/SIGTERM
"""

from __future__ import annotations

import asyncio
import logging
from datetime import datetime

from ..config import Settings
from ..connectors.malware import sync_malware_index
from ..storage.factory import create_graph_store
from ._base import CycleSummary, PreflightResult, WorkerHeartbeat, run_connector_loop

logger = logging.getLogger(__name__)


def _preflight(settings: Settings) -> PreflightResult:
    if not settings.malware_worker_enabled:
        return PreflightResult(ok=False, reason="MALWARE_WORKER_ENABLED=0")
    if not settings.elastic_connector_enabled:
        return PreflightResult(ok=False, reason="ELASTIC_CONNECTOR_ENABLED=0")
    if not settings.elastic_connector_hosts_list:
        return PreflightResult(ok=False, reason="no connector hosts configured")
    interval = settings.malware_worker_interval_minutes
    if interval <= 0:
        return PreflightResult(
            ok=False, reason=f"MALWARE_WORKER_INTERVAL_MINUTES={interval}"
        )
    indices = settings.malware_worker_indices_list
    if not indices:
        return PreflightResult(ok=False, reason="no malware indices configured")
    return PreflightResult(
        ok=True,
        interval_seconds=interval * 60,
        extra_heartbeat={"indices": indices},
    )


def _run_cycle(
    settings: Settings,
    since: datetime,
    until: datetime,
    heartbeat: WorkerHeartbeat,
) -> CycleSummary:
    indices = settings.malware_worker_indices_list
    graph_store = create_graph_store(settings)
    errors: list[str] = []

    for index_name in indices:
        try:
            result = sync_malware_index(
                settings=settings,
                graph_store=graph_store,
                index_name=index_name,
                since=since,
                until=until,
                max_docs=settings.malware_worker_max_per_index,
            )
            heartbeat.update(
                "running",
                {
                    "last_index": index_name,
                    "samples_processed": result.samples_processed,
                    "entities_created": result.entities_created,
                    "relations_created": result.relations_created,
                },
            )
            if result.errors:
                errors.extend(result.errors[:5])
        except Exception as exc:
            logger.exception("Malware sync failed for index %s", index_name)
            errors.append(f"{index_name}: {exc}")
            heartbeat.update("error", {"last_index": index_name})

    return CycleSummary(
        log_message=f"synced {len(indices)} indices",
        heartbeat_details={"indices": indices},
        errors=errors,
    )


async def malware_worker_loop() -> None:
    """Periodically sync malware samples from Elasticsearch."""
    await run_connector_loop(
        worker_name="malware-worker",
        preflight=_preflight,
        run_cycle=_run_cycle,
        lookback_minutes_fn=lambda s: s.malware_worker_lookback_minutes,
    )


def main() -> None:
    asyncio.run(malware_worker_loop())


if __name__ == "__main__":
    main()
