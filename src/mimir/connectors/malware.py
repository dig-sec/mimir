"""Malware feed → Mimir connector.

Reads malware sample documents from Elasticsearch indices
(``mwdb-openrelik``, ``dailymalwarefeed``) and imports structured
malware analysis data as graph entities and relationships.

Supported data sources
~~~~~~~~~~~~~~~~~~~~~~
* **mwdb-openrelik** — MWDB samples enriched with OpenRelik analysis
  (Capa, ExifTool, Strings, Yara, High-Entropy).
* **dailymalwarefeed** — Daily malware intelligence feed with IOCs
  and family attribution.

Design goals
~~~~~~~~~~~~
* **Structured-first**: extract entities directly from the already-parsed
  analysis artifacts (hashes, MITRE ATT&CK, Yara rules, IOCs) without
  needing an LLM pass.
* **Incremental**: use ``@timestamp`` / ``mwdb.upload_time`` for lookback.
* **Memory-constant**: page through ``search_after``.
* **Non-blocking**: all public functions are synchronous, designed to
  run inside ``asyncio.to_thread()``.
"""

from __future__ import annotations

import contextlib
import logging
import re
from dataclasses import dataclass, field
from datetime import datetime, timedelta, timezone
from typing import Any, Callable, Dict, Iterator, List, Optional, Tuple
from uuid import uuid4

from elasticsearch import Elasticsearch, NotFoundError

from ..config import Settings
from ..dedupe import EntityResolver
from ..normalize import canonical_entity_key
from ..schemas import Entity, Provenance, Relation
from ..storage.base import GraphStore
from ..utils.provenance import NS_PROVENANCE_MALWARE, det_prov_id
from ._shared import create_source_es_client

logger = logging.getLogger(__name__)

_NS_PROVENANCE = NS_PROVENANCE_MALWARE

# MITRE ATT&CK ID pattern
_MITRE_ID_RE = re.compile(r"[TtSs]\d{4}(?:\.\d{3})?|TA\d{4}")

# ── Tag parsing patterns ──────────────────────────────────────
_FAMILY_TAG_RE = re.compile(r"^family:(.+)$", re.IGNORECASE)
_SOURCE_TAG_RE = re.compile(r"^from:(.+)$", re.IGNORECASE)
_SYNC_TIMESTAMP_FIELDS = ("@timestamp", "mwdb.upload_time", "created_at", "updated_at")


# ── Results ──────────────────────────────────────────────────


@dataclass
class MalwareSyncResult:
    """Tracks statistics for a malware sync cycle."""

    indexes_scanned: int = 0
    samples_processed: int = 0
    entities_created: int = 0
    relations_created: int = 0
    skipped_existing: int = 0
    errors: List[str] = field(default_factory=list)


def _det_prov_id(
    *,
    source_uri: str,
    relation_id: str,
    model: str,
    chunk_id: str,
    start_offset: int,
    end_offset: int,
    snippet: str,
) -> str:
    """Deterministic provenance ID keyed by evidence granularity."""
    return det_prov_id(
        namespace=_NS_PROVENANCE,
        source_uri=source_uri,
        relation_id=relation_id,
        model=model,
        chunk_id=chunk_id,
        start_offset=start_offset,
        end_offset=end_offset,
        snippet=snippet,
    )


def _create_source_client(settings: Settings) -> Elasticsearch:
    """Create ES client pointing at the malware source indices."""
    return create_source_es_client(settings)


def _extract_field(doc: Dict[str, Any], dotted_path: str) -> Any:
    """Read a possibly nested dict value by dotted path."""
    node: Any = doc
    for part in dotted_path.split("."):
        if not isinstance(node, dict):
            return None
        node = node.get(part)
        if node is None:
            return None
    return node


def _parse_iso_datetime(value: Any) -> Optional[datetime]:
    """Best-effort parse of an ISO timestamp string."""
    if not isinstance(value, str) or not value.strip():
        return None
    try:
        parsed = datetime.fromisoformat(value.replace("Z", "+00:00"))
    except (TypeError, ValueError):
        return None
    if parsed.tzinfo is None:
        return parsed.replace(tzinfo=timezone.utc)
    return parsed.astimezone(timezone.utc)


def _first_timestamp(doc: Dict[str, Any], *field_paths: str) -> datetime:
    """Return first parseable timestamp across field candidates."""
    for field_path in field_paths:
        parsed = _parse_iso_datetime(_extract_field(doc, field_path))
        if parsed is not None:
            return parsed
    return datetime.now(timezone.utc)


def _normalize_attack_id(raw_id: str) -> str:
    """Normalize ATT&CK IDs to a stable uppercase representation."""
    candidate = (raw_id or "").strip().upper()
    if not candidate:
        return ""
    if _MITRE_ID_RE.fullmatch(candidate) is None:
        return ""
    return candidate


def _find_existing_sample_by_hash(
    graph_store: GraphStore, *hash_values: str
) -> Optional[Entity]:
    """Find an existing malware_sample entity by any known hash value."""
    for value in hash_values:
        if not value:
            continue
        matches = graph_store.search_entities(
            query=value,
            entity_type="malware_sample",
            canonical_key=canonical_entity_key(value, "malware_sample"),
        )
        if matches:
            return matches[0]
    return None


def _iter_malware_docs(
    client: Elasticsearch,
    index_name: str,
    since: datetime,
    settings: Settings,
    *,
    until: Optional[datetime] = None,
    max_docs: int = 500,
) -> Iterator[Dict[str, Any]]:
    """Page through malware documents using search_after."""
    page_size = settings.elastic_connector_page_size

    time_filter: Dict[str, Any] = {"gte": since.isoformat()}
    if until is not None:
        time_filter["lt"] = until.isoformat()

    time_ranges = [{"range": {field: time_filter}} for field in _SYNC_TIMESTAMP_FIELDS]
    query: Dict[str, Any] = {
        "bool": {
            "should": time_ranges,
            "minimum_should_match": 1,
        }
    }

    sort = [
        {
            "@timestamp": {
                "order": "desc",
                "unmapped_type": "date",
                "missing": "_last",
            }
        },
        {
            "mwdb.upload_time": {
                "order": "desc",
                "unmapped_type": "date",
                "missing": "_last",
            }
        },
        {"_shard_doc": "desc"},
    ]
    search_after = None
    yielded = 0

    while True:
        body: Dict[str, Any] = {
            "query": query,
            "sort": sort,
            "size": page_size,
        }
        if search_after:
            body["search_after"] = search_after

        try:
            resp = client.search(index=index_name, **body)
        except NotFoundError:
            logger.info("Index %s not found, skipping", index_name)
            return

        hits = resp.get("hits", {}).get("hits", [])
        if not hits:
            return

        for hit in hits:
            source = hit.get("_source", {})
            source["_doc_id"] = hit.get("_id", "unknown")
            yield source
            yielded += 1
            if 0 < max_docs <= yielded:
                return

        search_after = hits[-1]["sort"]


def _parse_tags(tags: List[str]) -> Tuple[List[str], List[str]]:
    """Parse MWDB-style tags into (families, sources)."""
    families: List[str] = []
    sources: List[str] = []
    for tag in tags:
        tag = tag.strip()
        if not tag:
            continue
        m_family = _FAMILY_TAG_RE.match(tag)
        if m_family:
            families.append(m_family.group(1).strip())
            continue
        m_source = _SOURCE_TAG_RE.match(tag)
        if m_source:
            sources.append(m_source.group(1).strip())
            continue
    return families, sources


def _create_relation_with_provenance(
    graph_store: GraphStore,
    *,
    subject_id: str,
    predicate: str,
    object_id: str,
    confidence: float,
    attrs: Dict[str, Any],
    source_uri: str,
    sync_run_id: str,
    snippet: str,
    timestamp: datetime,
    model: str = "malware-connector",
    prompt_version: str = "malware-sync-v1",
    result: MalwareSyncResult,
) -> None:
    """Create a relation with attached provenance."""
    rel = Relation(
        id=str(uuid4()),
        subject_id=subject_id,
        predicate=predicate,
        object_id=object_id,
        confidence=confidence,
        attrs=attrs,
    )
    stored = graph_store.upsert_relations([rel])[0]
    prov = Provenance(
        provenance_id=_det_prov_id(
            source_uri=source_uri,
            relation_id=stored.id,
            model=model,
            chunk_id=stored.id,
            start_offset=0,
            end_offset=0,
            snippet=snippet,
        ),
        source_uri=source_uri,
        chunk_id=stored.id,
        start_offset=0,
        end_offset=0,
        snippet=snippet,
        extraction_run_id=sync_run_id,
        model=model,
        prompt_version=prompt_version,
        timestamp=timestamp,
    )
    graph_store.attach_provenance(stored.id, prov)
    result.relations_created += 1


def _process_mwdb_sample(
    doc: Dict[str, Any],
    graph_store: GraphStore,
    resolver: EntityResolver,
    sync_run_id: str,
    result: MalwareSyncResult,
    index_name: str,
) -> None:
    """Process a single MWDB-OpenRelik malware sample into graph data.

    Creates:
    - A ``malware_sample`` entity for the binary (keyed by SHA-256)
    - ``malware`` family entities from tags
    - ``attack_pattern`` entities from Capa MITRE ATT&CK mappings
    - ``capa_rule`` entities from Capa behavioural capability names
    - ``capa_behavior`` entities from Capa MBC behavior mappings
    - ``yara_rule`` entities from Yara rule matches
    - ``indicator`` entities from extracted IOCs (IPs, URLs, emails)
    - Relationships linking the sample to families, techniques, IOCs
    """
    mwdb = doc.get("mwdb", {})
    sample = doc.get("sample", {})
    computed = doc.get("computed", {})
    artifacts = doc.get("artifacts", {})
    sha256 = mwdb.get("sha256") or sample.get("sha256", "")
    if not sha256:
        return

    published_dt = _first_timestamp(doc, "@timestamp", "mwdb.upload_time")

    source_uri = f"malware://{index_name}/{sha256}"
    file_name = mwdb.get("file_name") or sample.get("filename", sha256)

    # ── Create the malware sample entity ─────────────────────
    sample_entity = resolver.resolve(sha256, entity_type="malware_sample")
    sample_entity.attrs.update(
        {
            "origin": "malware-connector",
            "sha256": sha256,
            "sha1": mwdb.get("sha1", ""),
            "md5": mwdb.get("md5", ""),
            "ssdeep": mwdb.get("ssdeep", ""),
            "file_name": file_name,
            "file_size": mwdb.get("file_size"),
            "file_type": mwdb.get("file_type", ""),
            "source_index": index_name,
        }
    )
    if computed.get("imphash"):
        sample_entity.attrs["imphash"] = computed["imphash"]
    if mwdb.get("crc32"):
        sample_entity.attrs["crc32"] = mwdb["crc32"]
    if computed.get("compile_time"):
        sample_entity.attrs["compile_time"] = computed["compile_time"]
    if computed.get("entry_point"):
        sample_entity.attrs["entry_point"] = computed["entry_point"]
    if computed.get("sections"):
        sample_entity.attrs["pe_sections"] = computed["sections"]

    graph_store.upsert_entities([sample_entity])
    result.entities_created += 1

    # ── Parse tags: family attribution and source ──────────────
    tags = mwdb.get("tags", [])
    families, sources = _parse_tags(tags if isinstance(tags, list) else [])

    for family_name in families:
        family_entity = resolver.resolve(family_name, entity_type="malware")
        family_entity.attrs["origin"] = "malware-connector"
        graph_store.upsert_entities([family_entity])
        result.entities_created += 1

        _create_relation_with_provenance(
            graph_store,
            subject_id=sample_entity.id,
            predicate="variant_of",
            object_id=family_entity.id,
            confidence=0.90,
            attrs={"origin": "malware-connector", "source": "mwdb-tag"},
            source_uri=source_uri,
            sync_run_id=sync_run_id,
            snippet=f"Sample {sha256[:16]}… tagged as family:{family_name}",
            timestamp=published_dt,
            result=result,
        )

    for source_name in sources:
        source_entity = resolver.resolve(source_name, entity_type="identity")
        source_entity.attrs["origin"] = "malware-connector"
        source_entity.attrs["identity_class"] = "system"
        graph_store.upsert_entities([source_entity])
        result.entities_created += 1

        _create_relation_with_provenance(
            graph_store,
            subject_id=sample_entity.id,
            predicate="attributed_to",
            object_id=source_entity.id,
            confidence=0.70,
            attrs={"origin": "malware-connector", "source": "mwdb-tag"},
            source_uri=source_uri,
            sync_run_id=sync_run_id,
            snippet=f"Sample {sha256[:16]}… sourced from:{source_name}",
            timestamp=published_dt,
            result=result,
        )

    # ── Capa MITRE ATT&CK analysis ───────────────────────────
    capa_artifacts = artifacts.get("Capa Malware Analysis", [])
    if isinstance(capa_artifacts, list):
        for capa in capa_artifacts:
            summary = capa.get("summary", {}) if isinstance(capa, dict) else {}

            # ATT&CK technique mappings
            for mapping in summary.get("attack_mappings", []):
                technique_id = _normalize_attack_id(mapping.get("id", ""))
                technique_name = mapping.get("technique", "")
                tactic = mapping.get("tactic", "")
                subtechnique = mapping.get("subtechnique", "")

                if not technique_id:
                    continue

                display_name = technique_id
                if technique_name:
                    display_name = f"{technique_id} - {technique_name}"
                    if subtechnique:
                        display_name = (
                            f"{technique_id} - {technique_name}: {subtechnique}"
                        )

                technique_entity = resolver.resolve(
                    technique_id, entity_type="attack_pattern"
                )
                technique_entity.attrs["origin"] = "malware-connector"
                technique_entity.attrs["mitre_id"] = technique_id
                if tactic:
                    technique_entity.attrs["tactic"] = tactic
                if technique_name:
                    technique_entity.attrs["technique_name"] = technique_name
                if subtechnique:
                    technique_entity.attrs["subtechnique"] = subtechnique
                technique_entity.attrs["display_name"] = display_name
                graph_store.upsert_entities([technique_entity])
                result.entities_created += 1

                _create_relation_with_provenance(
                    graph_store,
                    subject_id=sample_entity.id,
                    predicate="uses",
                    object_id=technique_entity.id,
                    confidence=0.85,
                    attrs={
                        "origin": "malware-connector",
                        "source": "capa-analysis",
                        "tactic": tactic,
                    },
                    source_uri=source_uri,
                    sync_run_id=sync_run_id,
                    snippet=(
                        f"Capa detected {technique_id} ({technique_name}) "
                        f"in {file_name}"
                    ),
                    timestamp=published_dt,
                    result=result,
                )

            # Capa rule names → pivotable capa_rule entities
            for rule_name in summary.get("rule_names", []):
                if not rule_name or not rule_name.strip():
                    continue
                rule_name = rule_name.strip()
                rule_entity = resolver.resolve(rule_name, entity_type="capa_rule")
                rule_entity.attrs["origin"] = "malware-connector"
                rule_entity.attrs["rule_name"] = rule_name
                # Attach associated ATT&CK technique IDs for context
                technique_ids = [
                    _normalize_attack_id(m.get("id", ""))
                    for m in summary.get("attack_mappings", [])
                ]
                technique_ids = [t for t in technique_ids if t]
                if technique_ids:
                    rule_entity.attrs["mitre_ids"] = sorted(set(technique_ids))
                graph_store.upsert_entities([rule_entity])
                result.entities_created += 1

                _create_relation_with_provenance(
                    graph_store,
                    subject_id=sample_entity.id,
                    predicate="exhibits_capability",
                    object_id=rule_entity.id,
                    confidence=0.90,
                    attrs={
                        "origin": "malware-connector",
                        "source": "capa-analysis",
                    },
                    source_uri=source_uri,
                    sync_run_id=sync_run_id,
                    snippet=(f"Capa rule '{rule_name}' matched in {file_name}"),
                    timestamp=published_dt,
                    result=result,
                )

            # MBC (Malware Behavior Catalog) behaviors
            for mbc in summary.get("mbc_mappings", []):
                behavior = mbc.get("behavior", "")
                objective = mbc.get("objective", "")
                mbc_id = mbc.get("id", "")
                if not behavior:
                    continue

                mbc_label = (
                    f"MBC: {objective} - {behavior}"
                    if objective
                    else f"MBC: {behavior}"
                )
                mbc_entity = resolver.resolve(mbc_label, entity_type="capa_behavior")
                mbc_entity.attrs["origin"] = "malware-connector"
                mbc_entity.attrs["mbc_id"] = mbc_id
                mbc_entity.attrs["behavior"] = behavior
                mbc_entity.attrs["mbc_objective"] = objective
                graph_store.upsert_entities([mbc_entity])
                result.entities_created += 1

                _create_relation_with_provenance(
                    graph_store,
                    subject_id=sample_entity.id,
                    predicate="exhibits",
                    object_id=mbc_entity.id,
                    confidence=0.80,
                    attrs={
                        "origin": "malware-connector",
                        "source": "capa-mbc",
                    },
                    source_uri=source_uri,
                    sync_run_id=sync_run_id,
                    snippet=f"Capa MBC: {mbc_label} in {file_name}",
                    timestamp=published_dt,
                    result=result,
                )

    # ── Yara rule matches ────────────────────────────────────
    yara_artifacts = artifacts.get("Yara scan", [])
    if isinstance(yara_artifacts, list):
        for yara in yara_artifacts:
            summary = yara.get("summary", {}) if isinstance(yara, dict) else {}
            for rule_name in summary.get("rule_names", []):
                if not rule_name:
                    continue
                rule_entity = resolver.resolve(rule_name, entity_type="yara_rule")
                rule_entity.attrs["origin"] = "malware-connector"
                rule_entity.attrs["rule_name"] = rule_name
                graph_store.upsert_entities([rule_entity])
                result.entities_created += 1

                _create_relation_with_provenance(
                    graph_store,
                    subject_id=sample_entity.id,
                    predicate="matches",
                    object_id=rule_entity.id,
                    confidence=0.95,
                    attrs={
                        "origin": "malware-connector",
                        "source": "yara-scan",
                    },
                    source_uri=source_uri,
                    sync_run_id=sync_run_id,
                    snippet=f"Yara rule '{rule_name}' matched {file_name}",
                    timestamp=published_dt,
                    result=result,
                )

            # Yara tags (e.g. malware families detected by Yara)
            for yara_tag in summary.get("tags", []):
                if not yara_tag:
                    continue
                tag_entity = resolver.resolve(yara_tag, entity_type="malware")
                tag_entity.attrs["origin"] = "malware-connector"
                tag_entity.attrs["detection_source"] = "yara"
                graph_store.upsert_entities([tag_entity])
                result.entities_created += 1

                _create_relation_with_provenance(
                    graph_store,
                    subject_id=sample_entity.id,
                    predicate="detected_as",
                    object_id=tag_entity.id,
                    confidence=0.85,
                    attrs={
                        "origin": "malware-connector",
                        "source": "yara-tag",
                    },
                    source_uri=source_uri,
                    sync_run_id=sync_run_id,
                    snippet=f"Yara tag '{yara_tag}' on {file_name}",
                    timestamp=published_dt,
                    result=result,
                )

    # ── String IOC extraction ─────────────────────────────────
    strings_artifacts = artifacts.get("Strings", [])
    if isinstance(strings_artifacts, list):
        for strings_data in strings_artifacts:
            summary = (
                strings_data.get("summary", {})
                if isinstance(strings_data, dict)
                else {}
            )

            # IPs
            for ip in summary.get("ips", []):
                if not ip:
                    continue
                ip_entity = resolver.resolve(str(ip), entity_type="indicator")
                ip_entity.attrs["origin"] = "malware-connector"
                ip_entity.attrs["indicator_type"] = "ip"
                graph_store.upsert_entities([ip_entity])
                result.entities_created += 1

                _create_relation_with_provenance(
                    graph_store,
                    subject_id=sample_entity.id,
                    predicate="communicates_with",
                    object_id=ip_entity.id,
                    confidence=0.75,
                    attrs={
                        "origin": "malware-connector",
                        "source": "strings-extraction",
                    },
                    source_uri=source_uri,
                    sync_run_id=sync_run_id,
                    snippet=f"IP {ip} extracted from strings of {file_name}",
                    timestamp=published_dt,
                    result=result,
                )

            # URLs
            for url in summary.get("urls", []):
                if not url:
                    continue
                url_entity = resolver.resolve(url, entity_type="indicator")
                url_entity.attrs["origin"] = "malware-connector"
                url_entity.attrs["indicator_type"] = "url"
                graph_store.upsert_entities([url_entity])
                result.entities_created += 1

                _create_relation_with_provenance(
                    graph_store,
                    subject_id=sample_entity.id,
                    predicate="communicates_with",
                    object_id=url_entity.id,
                    confidence=0.70,
                    attrs={
                        "origin": "malware-connector",
                        "source": "strings-extraction",
                    },
                    source_uri=source_uri,
                    sync_run_id=sync_run_id,
                    snippet=f"URL {url[:80]} extracted from strings of {file_name}",
                    timestamp=published_dt,
                    result=result,
                )

            # Emails
            for email in summary.get("emails", []):
                if not email:
                    continue
                email_entity = resolver.resolve(email, entity_type="indicator")
                email_entity.attrs["origin"] = "malware-connector"
                email_entity.attrs["indicator_type"] = "email"
                graph_store.upsert_entities([email_entity])
                result.entities_created += 1

                _create_relation_with_provenance(
                    graph_store,
                    subject_id=sample_entity.id,
                    predicate="communicates_with",
                    object_id=email_entity.id,
                    confidence=0.65,
                    attrs={
                        "origin": "malware-connector",
                        "source": "strings-extraction",
                    },
                    source_uri=source_uri,
                    sync_run_id=sync_run_id,
                    snippet=f"Email {email} extracted from strings of {file_name}",
                    timestamp=published_dt,
                    result=result,
                )

    # ── ExifTool metadata → infrastructure entity ────────────
    exiftool_artifacts = artifacts.get("ExifTool Extractor", [])
    if isinstance(exiftool_artifacts, list):
        for exif in exiftool_artifacts:
            summary = exif.get("summary", {}) if isinstance(exif, dict) else {}
            pdb_path = summary.get("pdb_path", "")
            if pdb_path:
                sample_entity.attrs["pdb_path"] = pdb_path
            company = summary.get("company_name", "")
            if company:
                sample_entity.attrs["company_name"] = company
            original_fn = summary.get("original_filename", "")
            if original_fn:
                sample_entity.attrs["original_filename"] = original_fn
            pe_arch = summary.get("machine_type", "")
            if pe_arch:
                sample_entity.attrs["pe_arch"] = pe_arch

    # ── High entropy detection ───────────────────────────────
    entropy_artifacts = artifacts.get("High Entropy", [])
    if isinstance(entropy_artifacts, list):
        for ent in entropy_artifacts:
            summary = ent.get("summary", {}) if isinstance(ent, dict) else {}
            if summary.get("has_high_entropy"):
                sample_entity.attrs["has_high_entropy"] = True
                entropy_val = summary.get("entropy_value")
                if entropy_val is not None:
                    sample_entity.attrs["entropy_value"] = entropy_val
                sections = summary.get("high_entropy_sections", [])
                if sections:
                    sample_entity.attrs["high_entropy_sections"] = sections

    # Persist any attribute updates from ExifTool / High Entropy
    graph_store.upsert_entities([sample_entity])

    # ── Cross-link malware families to ATT&CK techniques ─────
    # If we have both families and attack_patterns, link them
    if families:
        # Gather technique entities we created above
        for family_name in families:
            family_entity = resolver.resolve(family_name, entity_type="malware")
            for capa in (
                artifacts.get("Capa Malware Analysis", [])
                if isinstance(artifacts.get("Capa Malware Analysis"), list)
                else []
            ):
                summary = capa.get("summary", {}) if isinstance(capa, dict) else {}
                seen_techniques: set = set()
                for mapping in summary.get("attack_mappings", []):
                    technique_id = mapping.get("id", "")
                    technique_id = _normalize_attack_id(technique_id)
                    if not technique_id or technique_id in seen_techniques:
                        continue
                    seen_techniques.add(technique_id)
                    technique_entity = resolver.resolve(
                        technique_id, entity_type="attack_pattern"
                    )
                    _create_relation_with_provenance(
                        graph_store,
                        subject_id=family_entity.id,
                        predicate="uses",
                        object_id=technique_entity.id,
                        confidence=0.75,
                        attrs={
                            "origin": "malware-connector",
                            "inference": "family-technique-link",
                        },
                        source_uri=source_uri,
                        sync_run_id=sync_run_id,
                        snippet=(
                            f"Family {family_name} uses {technique_id} "
                            f"(observed in sample {sha256[:16]}…)"
                        ),
                        timestamp=published_dt,
                        result=result,
                    )

    result.samples_processed += 1


def _process_dailymalwarefeed_sample(
    doc: Dict[str, Any],
    graph_store: GraphStore,
    resolver: EntityResolver,
    sync_run_id: str,
    result: MalwareSyncResult,
    index_name: str,
) -> None:
    """Process a dailymalwarefeed document into graph data.

    This handler is more generic since the dailymalwarefeed index
    may not exist yet. It uses common malware-feed field names
    and falls back to the MWDB handler if the doc structure matches.
    """
    # If the document looks like an MWDB doc (has "mwdb" key), use that handler
    if "mwdb" in doc:
        _process_mwdb_sample(
            doc, graph_store, resolver, sync_run_id, result, index_name
        )
        return

    published_dt = _first_timestamp(
        doc,
        "@timestamp",
        "mwdb.upload_time",
        "upload_time",
        "published_at",
    )

    # Try to find hashes in common field locations.
    sha256 = (
        doc.get("sha256")
        or doc.get("hash", {}).get("sha256")
        or doc.get("file", {}).get("hash", {}).get("sha256")
        or doc.get("file_sha256")
        or ""
    )
    md5 = (
        doc.get("md5")
        or doc.get("hash", {}).get("md5")
        or doc.get("file", {}).get("hash", {}).get("md5")
        or ""
    )
    sha1 = (
        doc.get("sha1")
        or doc.get("hash", {}).get("sha1")
        or doc.get("file", {}).get("hash", {}).get("sha1")
        or ""
    )

    sample_key = sha256 or md5 or sha1
    if not sample_key:
        return

    source_uri = f"malware://{index_name}/{sample_key}"
    file_name = (
        doc.get("file_name")
        or doc.get("filename")
        or doc.get("file", {}).get("name", "")
        or sample_key
    )

    # ── Create sample entity ─────────────────────────────────
    sample_entity = _find_existing_sample_by_hash(graph_store, sha256, md5, sha1)
    if sample_entity is None:
        sample_entity = resolver.resolve(sample_key, entity_type="malware_sample")
    else:
        if sample_key and sample_key != sample_entity.name:
            aliases = {sample_entity.name, *sample_entity.aliases}
            aliases.discard(sample_key)
            sample_entity.name = sample_key
            sample_entity.aliases = sorted(aliases)

    aliases = set(sample_entity.aliases)
    for candidate in (sha256, sha1, md5):
        if candidate and candidate != sample_entity.name:
            aliases.add(candidate)
    sample_entity.aliases = sorted(aliases)

    sample_entity.attrs["origin"] = "malware-connector"
    sample_entity.attrs["source_index"] = index_name
    sample_entity.attrs["file_name"] = file_name

    # Populate hash fields from whichever source provides them
    for hash_field in ("sha256", "sha1", "md5", "ssdeep"):
        value = (
            doc.get(hash_field)
            or doc.get("hash", {}).get(hash_field)
            or doc.get("file", {}).get("hash", {}).get(hash_field, "")
        )
        if value:
            sample_entity.attrs[hash_field] = value

    file_type = doc.get("file_type") or doc.get("type", "")
    if file_type:
        sample_entity.attrs["file_type"] = file_type

    file_size = doc.get("file_size") or doc.get("size")
    if file_size is not None:
        sample_entity.attrs["file_size"] = file_size

    graph_store.upsert_entities([sample_entity])
    result.entities_created += 1

    # ── Malware family ────────────────────────────────────────
    family = doc.get("malware_family") or doc.get("family", "")
    if family:
        family_entity = resolver.resolve(family, entity_type="malware")
        family_entity.attrs["origin"] = "malware-connector"
        graph_store.upsert_entities([family_entity])
        result.entities_created += 1

        _create_relation_with_provenance(
            graph_store,
            subject_id=sample_entity.id,
            predicate="variant_of",
            object_id=family_entity.id,
            confidence=0.85,
            attrs={"origin": "malware-connector", "source": "dailymalwarefeed"},
            source_uri=source_uri,
            sync_run_id=sync_run_id,
            snippet=f"Sample {sample_key[:16]}… classified as {family}",
            timestamp=published_dt,
            result=result,
        )

    # ── Tags (may contain family labels) ──────────────────────
    tags = doc.get("tags", [])
    if isinstance(tags, list):
        families_from_tags, sources_from_tags = _parse_tags(tags)
        for fam in families_from_tags:
            fam_entity = resolver.resolve(fam, entity_type="malware")
            fam_entity.attrs["origin"] = "malware-connector"
            graph_store.upsert_entities([fam_entity])
            result.entities_created += 1

            _create_relation_with_provenance(
                graph_store,
                subject_id=sample_entity.id,
                predicate="variant_of",
                object_id=fam_entity.id,
                confidence=0.85,
                attrs={"origin": "malware-connector", "source": "feed-tag"},
                source_uri=source_uri,
                sync_run_id=sync_run_id,
                snippet=f"Sample {sample_key[:16]}… tagged as family:{fam}",
                timestamp=published_dt,
                result=result,
            )
        for source_name in sources_from_tags:
            source_entity = resolver.resolve(source_name, entity_type="identity")
            source_entity.attrs["origin"] = "malware-connector"
            source_entity.attrs["identity_class"] = "system"
            graph_store.upsert_entities([source_entity])
            result.entities_created += 1

            _create_relation_with_provenance(
                graph_store,
                subject_id=sample_entity.id,
                predicate="attributed_to",
                object_id=source_entity.id,
                confidence=0.70,
                attrs={"origin": "malware-connector", "source": "feed-tag"},
                source_uri=source_uri,
                sync_run_id=sync_run_id,
                snippet=f"Sample {sample_key[:16]}… sourced from:{source_name}",
                timestamp=published_dt,
                result=result,
            )

    # ── IOCs (IPs, URLs, domains) ────────────────────────────
    for ioc_field, ioc_type, predicate in [
        ("iocs.ips", "ip", "communicates_with"),
        ("iocs.urls", "url", "communicates_with"),
        ("iocs.domains", "domain", "communicates_with"),
        ("iocs.emails", "email", "communicates_with"),
    ]:
        parts = ioc_field.split(".")
        container = doc
        for part in parts:
            if isinstance(container, dict):
                container = container.get(part, {})
            else:
                container = None
                break

        if isinstance(container, list):
            for ioc in container:
                if not ioc:
                    continue
                ioc_entity = resolver.resolve(str(ioc), entity_type="indicator")
                ioc_entity.attrs["origin"] = "malware-connector"
                ioc_entity.attrs["indicator_type"] = ioc_type
                graph_store.upsert_entities([ioc_entity])
                result.entities_created += 1

                _create_relation_with_provenance(
                    graph_store,
                    subject_id=sample_entity.id,
                    predicate=predicate,
                    object_id=ioc_entity.id,
                    confidence=0.70,
                    attrs={
                        "origin": "malware-connector",
                        "source": "dailymalwarefeed",
                    },
                    source_uri=source_uri,
                    sync_run_id=sync_run_id,
                    snippet=f"{ioc_type.upper()} {ioc} from {file_name}",
                    timestamp=published_dt,
                    result=result,
                )

    result.samples_processed += 1


# ── Index handler routing ─────────────────────────────────────

_INDEX_HANDLERS = {
    "mwdb-openrelik": _process_mwdb_sample,
    "dailymalwarefeed": _process_dailymalwarefeed_sample,
}


def sync_malware_index(
    settings: Settings,
    graph_store: GraphStore,
    *,
    index_name: str,
    since: Optional[datetime] = None,
    until: Optional[datetime] = None,
    max_docs: int = 0,
    progress_cb: Optional[Callable[[str], None]] = None,
) -> MalwareSyncResult:
    """Pull malware samples from an ES index and import as graph data.

    Parameters
    ----------
    settings : Settings
        Application settings (ES connection details).
    graph_store : GraphStore
        Where to upsert entities / relations.
    index_name : str
        Elasticsearch index to read from.
    since : datetime, optional
        Only fetch docs newer than ``since`` on supported timestamp fields.
    until : datetime, optional
        Optional upper bound (exclusive) on those timestamp fields.
    max_docs : int
        Cap on documents per index (0 = use config default).
    progress_cb : callable, optional
        ``progress_cb(message)`` for progress reporting.
    """
    result = MalwareSyncResult()
    resolver = EntityResolver(graph_store)
    sync_run_id = f"malware-sync-{uuid4()}"

    client = _create_source_client(settings)

    if since is None:
        since = datetime.now(timezone.utc) - timedelta(
            minutes=settings.malware_worker_lookback_minutes
        )

    if max_docs <= 0:
        max_docs = settings.malware_worker_max_per_index

    handler = _INDEX_HANDLERS.get(index_name, _process_dailymalwarefeed_sample)

    def _progress(msg: str) -> None:
        if progress_cb:
            progress_cb(msg)

    _progress(f"Scanning {index_name} since {since.isoformat()}...")

    # Check if the index exists before trying to query it
    try:
        if not client.indices.exists(index=index_name):
            _progress(f"Index {index_name} does not exist — skipping")
            logger.info("Malware sync: index %s does not exist, skipping", index_name)
            client.close()
            return result
    except Exception:
        pass  # proceed and let search report a proper error

    # Use bulk_mode if available for throughput
    bulk_ctx = (
        graph_store.bulk_mode()
        if hasattr(graph_store, "bulk_mode")
        else contextlib.nullcontext()
    )

    try:
        with bulk_ctx:
            for i, doc in enumerate(
                _iter_malware_docs(
                    client,
                    index_name,
                    since,
                    settings,
                    until=until,
                    max_docs=max_docs,
                )
            ):
                try:
                    handler(
                        doc,
                        graph_store,
                        resolver,
                        sync_run_id,
                        result,
                        index_name,
                    )
                except Exception as exc:
                    sha = (
                        doc.get("mwdb", {}).get("sha256")
                        or doc.get("sha256")
                        or doc.get("_doc_id", "?")
                    )
                    result.errors.append(f"{sha[:32]}: {exc}")
                    logger.warning("Failed to process malware sample: %s", exc)

                if (i + 1) % 25 == 0:
                    _progress(
                        f"Processed {i+1} samples: "
                        f"{result.entities_created} entities, "
                        f"{result.relations_created} relations"
                    )
    finally:
        client.close()

    result.indexes_scanned = 1

    logger.info(
        "Malware sync %s complete: %d samples, %d entities, %d relations, %d errors",
        index_name,
        result.samples_processed,
        result.entities_created,
        result.relations_created,
        len(result.errors),
    )
    return result
